{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis - STS Dataset\n",
    "\n",
    "Dataset being used for this example is the SEMEVAL-2012 Semantic Textual Similarity (STS) task, available from the [Semantic Textual Similarity Wiki page](http://ixa2.si.ehu.es/stswiki/index.php/Main_Page).\n",
    "\n",
    "There are 3 sets of sentence pairs and associated scores in the training set, as listed below:\n",
    "\n",
    "* 750 sentence pairs from Microsoft Research Paraphrase Corpus (MSR-Paraphrase)\n",
    "* 750 sentence pairs from Microsoft Research Video Description Corpus (MSR-Video)\n",
    "* 734 sentence pairs from [WMT2008 Development Dataset]( http://www.statmt.org/wmt08/shared-evaluation-task.html).\n",
    "\n",
    "Sentence pairs are available in one .txt file and associated labels (real values from 0-5) are available in the corresponding .gs file.\n",
    "\n",
    "The test set consists of 750, 750 and 459 different sentence pairs from these same 3 corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import collections\n",
    "import nltk\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test-gold\")\n",
    "\n",
    "DATA_SOURCES = [\"MSRpar\", \"MSRvid\", \"SMTeuroparl\"]\n",
    "SPAIR_FILE_TPL = \"STS.input.{:s}.txt\"\n",
    "LABEL_FILE_TPL = \"STS.gs.{:s}.txt\"\n",
    "\n",
    "VOCAB_FILE = os.path.join(DATA_DIR, \"sts-vocab.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2234, 2234, 2234) (1959, 1959, 1959)\n"
     ]
    }
   ],
   "source": [
    "def load_data(datadir):\n",
    "    xleft, xright, ys = [], [], []\n",
    "    for data_source in DATA_SOURCES:\n",
    "        label_filename = LABEL_FILE_TPL.format(data_source)\n",
    "        flabel = open(os.path.join(datadir, label_filename))\n",
    "        for line in flabel:\n",
    "            ys.append(float(line.strip()))\n",
    "        flabel.close()\n",
    "        # sentence pairs\n",
    "        spair_filename = SPAIR_FILE_TPL.format(data_source)\n",
    "        fsents = open(os.path.join(datadir, spair_filename))\n",
    "        for line in fsents:\n",
    "            left, right = line.strip().split(\"\\t\")\n",
    "            xleft.append(left)\n",
    "            xright.append(right)\n",
    "        fsents.close()\n",
    "        assert len(xleft) == len(xright) and len(xright) == len(ys)\n",
    "    return xleft, xright, ys\n",
    "    \n",
    "xleft_train, xright_train, ys_train = load_data(TRAIN_DIR)\n",
    "xleft_test, xright_test, ys_test = load_data(TEST_DIR)\n",
    "\n",
    "print((len(xleft_train), len(xright_train), len(ys_train)),\n",
    "      (len(xleft_test), len(xright_test), len(ys_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for sent_coll in [xleft_train, xright_train, xleft_test, xright_test]:\n",
    "    for sent in sent_coll:\n",
    "        sent = sent.decode(\"utf8\").encode(\"ascii\", \"ignore\").lower()\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            counter[word] += 1\n",
    "\n",
    "fvocab = open(VOCAB_FILE, \"wb\")\n",
    "for word, count in counter.most_common():\n",
    "    fvocab.write(\"{:s}\\t{:d}\\n\".format(word, count))\n",
    "fvocab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6940\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len([w for w, c in counter.most_common() if c >= 2])\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute #-words per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_words_list = []\n",
    "for sent_coll in [xleft_train, xright_train, xleft_test, xright_test]:\n",
    "    for sent in sent_coll:\n",
    "        sent = sent.decode(\"utf8\").encode(\"ascii\", \"ignore\").lower()\n",
    "        num_words_list.append(len([w for w in nltk.word_tokenize(sent)]))\n",
    "num_words = np.array(num_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.hist(num_words, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 percentile, #-sentences: 30\n",
      "91 percentile, #-sentences: 30\n",
      "92 percentile, #-sentences: 31\n",
      "93 percentile, #-sentences: 32\n",
      "94 percentile, #-sentences: 34\n",
      "95 percentile, #-sentences: 36\n",
      "96 percentile, #-sentences: 39\n",
      "97 percentile, #-sentences: 44\n",
      "98 percentile, #-sentences: 50\n",
      "99 percentile, #-sentences: 60\n"
     ]
    }
   ],
   "source": [
    "for i in range(90, 100):\n",
    "    print(\"{:d} percentile, #-sentences: {:.0f}\".format(\n",
    "        i, np.percentile(num_words, i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4193,) 0 5\n"
     ]
    }
   ],
   "source": [
    "y = np.array(ys_train + ys_test, dtype=\"int32\")\n",
    "print(y.shape, min(y), max(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
